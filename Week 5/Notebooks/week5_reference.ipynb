{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprince0031/ICT-Python-ML/blob/main/Week%205/Notebooks/week5_reference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Python & ML Foundations: Session 5\n",
        "## Perceptrons, MLPs & Neural Networks\n",
        "\n",
        "Welcome to the session 5 reference notebook! This week, we dive into the foundations of neural networks. We'll start with the simplest neural network unit—the perceptron, build up to multi-layer perceptrons (MLPs), explore advanced evaluation metrics, and understand how deep neural networks learn.\n",
        "\n",
        "**Libraries for this week:**\n",
        "- `numpy`: For numerical operations.\n",
        "- `pandas`: For data manipulation.\n",
        "- `matplotlib` & `seaborn`: For visualization.\n",
        "- `scikit-learn`: For building and evaluating models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_cell"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.datasets import make_classification, make_moons\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "separator_1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video1_header"
      },
      "source": [
        "## Video 1: Perceptron & MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perceptron_intro"
      },
      "source": [
        "### 1.1 - What is a Perceptron?\n",
        "\n",
        "The perceptron is the simplest form of a neural network—a single neuron that makes binary classifications. It takes multiple inputs, multiplies them by weights, adds a bias, and passes the result through an activation function.\n",
        "\n",
        "**Key components:**\n",
        "- Inputs: Features from our data\n",
        "- Weights: Learned parameters that determine feature importance\n",
        "- Bias: A constant that shifts the decision boundary\n",
        "- Activation function: Determines the output (step function for perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perceptron_data"
      },
      "outputs": [],
      "source": [
        "# Create a simple linearly separable dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, \n",
        "                          flip_y=0, random_state=42)\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0', alpha=0.6)\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1', alpha=0.6)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Binary Classification Dataset')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perceptron_train_test"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "perceptron_model"
      },
      "outputs": [],
      "source": [
        "# Train a perceptron\n",
        "perceptron = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = perceptron.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Perceptron Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Weights: {perceptron.coef_}\")\n",
        "print(f\"Bias: {perceptron.intercept_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlp_intro"
      },
      "source": [
        "### 1.2 - Multi-Layer Perceptron (MLP)\n",
        "\n",
        "While a single perceptron can only learn linear decision boundaries, an MLP can learn complex, non-linear patterns by stacking multiple layers of neurons.\n",
        "\n",
        "**Architecture:**\n",
        "- Input layer: Receives the features\n",
        "- Hidden layer(s): Process the information\n",
        "- Output layer: Makes predictions\n",
        "\n",
        "Each layer applies weights, biases, and activation functions to transform the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlp_nonlinear_data"
      },
      "outputs": [],
      "source": [
        "# Create a non-linear dataset (moons)\n",
        "X_moons, y_moons = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], c='blue', label='Class 0', alpha=0.6)\n",
        "plt.scatter(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], c='red', label='Class 1', alpha=0.6)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Non-Linear Classification Dataset (Moons)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlp_prepare"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_moons, y_moons, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler_m = StandardScaler()\n",
        "X_train_m_scaled = scaler_m.fit_transform(X_train_m)\n",
        "X_test_m_scaled = scaler_m.transform(X_test_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlp_simple"
      },
      "outputs": [],
      "source": [
        "# Create a simple MLP with one hidden layer\n",
        "mlp_simple = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', \n",
        "                          max_iter=1000, random_state=42)\n",
        "mlp_simple.fit(X_train_m_scaled, y_train_m)\n",
        "\n",
        "# Predictions\n",
        "y_pred_mlp = mlp_simple.predict(X_test_m_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy_mlp = accuracy_score(y_test_m, y_pred_mlp)\n",
        "print(f\"Simple MLP Accuracy: {accuracy_mlp:.4f}\")\n",
        "print(f\"Number of layers: {mlp_simple.n_layers_}\")\n",
        "print(f\"Number of iterations: {mlp_simple.n_iter_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlp_comparison"
      },
      "outputs": [],
      "source": [
        "# Compare perceptron vs MLP on non-linear data\n",
        "perceptron_nl = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron_nl.fit(X_train_m_scaled, y_train_m)\n",
        "y_pred_perceptron = perceptron_nl.predict(X_test_m_scaled)\n",
        "\n",
        "acc_perceptron = accuracy_score(y_test_m, y_pred_perceptron)\n",
        "acc_mlp = accuracy_score(y_test_m, y_pred_mlp)\n",
        "\n",
        "print(f\"Perceptron Accuracy on Non-linear Data: {acc_perceptron:.4f}\")\n",
        "print(f\"MLP Accuracy on Non-linear Data: {acc_mlp:.4f}\")\n",
        "print(f\"\\nImprovement with MLP: {(acc_mlp - acc_perceptron) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "separator_2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video2_header"
      },
      "source": [
        "## Video 2: MLPs 2 & Advanced Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlp_deep_intro"
      },
      "source": [
        "### 2.1 - Deeper MLPs\n",
        "\n",
        "Adding more hidden layers and neurons allows the network to learn more complex patterns. However, we need to be careful about overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlp_architectures"
      },
      "outputs": [],
      "source": [
        "# Let's try different architectures\n",
        "architectures = [\n",
        "    (10,),           # 1 hidden layer, 10 neurons\n",
        "    (20, 10),        # 2 hidden layers\n",
        "    (30, 20, 10),    # 3 hidden layers\n",
        "    (50, 25),        # 2 hidden layers, more neurons\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for arch in architectures:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=arch, activation='relu', \n",
        "                       max_iter=1000, random_state=42)\n",
        "    mlp.fit(X_train_m_scaled, y_train_m)\n",
        "    y_pred = mlp.predict(X_test_m_scaled)\n",
        "    acc = accuracy_score(y_test_m, y_pred)\n",
        "    results.append({'architecture': arch, 'accuracy': acc})\n",
        "    print(f\"Architecture {arch}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "# Find the best architecture\n",
        "best = max(results, key=lambda x: x['accuracy'])\n",
        "print(f\"\\nBest Architecture: {best['architecture']} with accuracy {best['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "activation_functions"
      },
      "source": [
        "### 2.2 - Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns.\n",
        "\n",
        "Common activation functions:\n",
        "- **ReLU** (Rectified Linear Unit): Most popular, fast to compute\n",
        "- **Sigmoid**: Outputs between 0 and 1, good for binary classification output\n",
        "- **Tanh**: Outputs between -1 and 1, zero-centered\n",
        "- **Identity**: Linear activation (rarely used in hidden layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "activation_comparison"
      },
      "outputs": [],
      "source": [
        "# Compare different activation functions\n",
        "activations = ['relu', 'tanh', 'logistic']\n",
        "\n",
        "for activation in activations:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(20, 10), activation=activation,\n",
        "                       max_iter=1000, random_state=42)\n",
        "    mlp.fit(X_train_m_scaled, y_train_m)\n",
        "    y_pred = mlp.predict(X_test_m_scaled)\n",
        "    acc = accuracy_score(y_test_m, y_pred)\n",
        "    print(f\"Activation: {activation:10s} - Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_metrics_intro"
      },
      "source": [
        "### 2.3 - Beyond Accuracy: Advanced Evaluation Metrics\n",
        "\n",
        "Accuracy alone doesn't tell the full story, especially with imbalanced datasets. We need to understand:\n",
        "\n",
        "- **Precision**: Of all positive predictions, how many were correct?\n",
        "- **Recall**: Of all actual positives, how many did we find?\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **Confusion Matrix**: Shows all prediction outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_imbalanced_data"
      },
      "outputs": [],
      "source": [
        "# Create an imbalanced dataset\n",
        "X_imb, y_imb = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                                   n_redundant=5, n_classes=2, weights=[0.9, 0.1],\n",
        "                                   random_state=42)\n",
        "\n",
        "print(f\"Class distribution:\")\n",
        "unique, counts = np.unique(y_imb, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"  Class {cls}: {count} samples ({count/len(y_imb)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_on_imbalanced"
      },
      "outputs": [],
      "source": [
        "# Split and scale\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb)\n",
        "\n",
        "scaler_imb = StandardScaler()\n",
        "X_train_imb_scaled = scaler_imb.fit_transform(X_train_imb)\n",
        "X_test_imb_scaled = scaler_imb.transform(X_test_imb)\n",
        "\n",
        "# Train MLP\n",
        "mlp_imb = MLPClassifier(hidden_layer_sizes=(50, 25), activation='relu',\n",
        "                        max_iter=1000, random_state=42)\n",
        "mlp_imb.fit(X_train_imb_scaled, y_train_imb)\n",
        "\n",
        "# Predictions\n",
        "y_pred_imb = mlp_imb.predict(X_test_imb_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_with_metrics"
      },
      "outputs": [],
      "source": [
        "# Calculate various metrics\n",
        "accuracy = accuracy_score(y_test_imb, y_pred_imb)\n",
        "precision = precision_score(y_test_imb, y_pred_imb)\n",
        "recall = recall_score(y_test_imb, y_pred_imb)\n",
        "f1 = f1_score(y_test_imb, y_pred_imb)\n",
        "\n",
        "print(\"Model Performance on Imbalanced Data:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion_matrix_viz"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_imb, y_pred_imb)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Breakdown:\")\n",
        "print(f\"  True Negatives:  {cm[0, 0]}\")\n",
        "print(f\"  False Positives: {cm[0, 1]}\")\n",
        "print(f\"  False Negatives: {cm[1, 0]}\")\n",
        "print(f\"  True Positives:  {cm[1, 1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "classification_report"
      },
      "outputs": [],
      "source": [
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_imb, y_pred_imb, target_names=['Class 0', 'Class 1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "separator_3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video3_header"
      },
      "source": [
        "## Video 3: Neural Networks Deep Dive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn_learning_intro"
      },
      "source": [
        "### 3.1 - How Neural Networks Learn\n",
        "\n",
        "Neural networks learn through a process called **backpropagation**:\n",
        "\n",
        "1. **Forward Pass**: Data flows through the network, producing predictions\n",
        "2. **Loss Calculation**: Compare predictions to actual values\n",
        "3. **Backward Pass**: Calculate gradients (how much each weight contributed to the error)\n",
        "4. **Weight Update**: Adjust weights to reduce error\n",
        "5. **Repeat**: Continue until the model converges\n",
        "\n",
        "This process is controlled by hyperparameters like learning rate and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "learning_rate_comparison"
      },
      "outputs": [],
      "source": [
        "# Effect of learning rate\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
        "\n",
        "print(\"Effect of Learning Rate on Model Performance:\\n\")\n",
        "for lr in learning_rates:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(20, 10), activation='relu',\n",
        "                       learning_rate_init=lr, max_iter=1000, random_state=42)\n",
        "    mlp.fit(X_train_m_scaled, y_train_m)\n",
        "    y_pred = mlp.predict(X_test_m_scaled)\n",
        "    acc = accuracy_score(y_test_m, y_pred)\n",
        "    print(f\"  Learning Rate {lr:6.3f}: Accuracy = {acc:.4f}, Iterations = {mlp.n_iter_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solvers_intro"
      },
      "source": [
        "### 3.2 - Optimization Algorithms (Solvers)\n",
        "\n",
        "Different optimization algorithms update weights in different ways:\n",
        "\n",
        "- **SGD** (Stochastic Gradient Descent): Updates weights after each sample\n",
        "- **Adam**: Adaptive learning rate, combines momentum and RMSprop\n",
        "- **L-BFGS**: Quasi-Newton method, good for small datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "solver_comparison"
      },
      "outputs": [],
      "source": [
        "# Compare different solvers\n",
        "solvers = ['sgd', 'adam', 'lbfgs']\n",
        "\n",
        "print(\"Comparison of Different Optimization Algorithms:\\n\")\n",
        "for solver in solvers:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(30, 20), activation='relu',\n",
        "                       solver=solver, max_iter=1000, random_state=42)\n",
        "    mlp.fit(X_train_m_scaled, y_train_m)\n",
        "    y_pred = mlp.predict(X_test_m_scaled)\n",
        "    acc = accuracy_score(y_test_m, y_pred)\n",
        "    print(f\"  Solver: {solver:8s} - Accuracy: {acc:.4f}, Iterations: {mlp.n_iter_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regularization_intro"
      },
      "source": [
        "### 3.3 - Regularization to Prevent Overfitting\n",
        "\n",
        "As networks get deeper, they can overfit the training data. Regularization helps by:\n",
        "\n",
        "- **L2 Regularization (alpha parameter)**: Penalizes large weights\n",
        "- **Early Stopping**: Stops training when validation performance stops improving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "regularization_demo"
      },
      "outputs": [],
      "source": [
        "# Effect of regularization\n",
        "alphas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
        "\n",
        "print(\"Effect of L2 Regularization (alpha parameter):\\n\")\n",
        "for alpha in alphas:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(50, 30, 20), activation='relu',\n",
        "                       alpha=alpha, max_iter=1000, random_state=42)\n",
        "    mlp.fit(X_train_m_scaled, y_train_m)\n",
        "    \n",
        "    train_acc = mlp.score(X_train_m_scaled, y_train_m)\n",
        "    test_acc = mlp.score(X_test_m_scaled, y_test_m)\n",
        "    \n",
        "    print(f\"  Alpha {alpha:7.4f}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}, Gap = {train_acc - test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_size_intro"
      },
      "source": [
        "### 3.4 - Batch Size and Training Dynamics\n",
        "\n",
        "The batch size determines how many samples are used to compute each gradient update:\n",
        "\n",
        "- **Small batches**: Noisy gradients, but can escape local minima\n",
        "- **Large batches**: Stable gradients, but may get stuck\n",
        "- **Mini-batch**: Sweet spot between the two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_size_comparison"
      },
      "outputs": [],
      "source": [
        "# Compare different batch sizes\n",
        "batch_sizes = [32, 64, 128, 200]\n",
        "\n",
        "print(\"Effect of Batch Size on Training:\\n\")\n",
        "for batch_size in batch_sizes:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(20, 10), activation='relu',\n",
        "                       batch_size=batch_size, max_iter=1000, random_state=42)\n",
        "    mlp.fit(X_train_m_scaled, y_train_m)\n",
        "    y_pred = mlp.predict(X_test_m_scaled)\n",
        "    acc = accuracy_score(y_test_m, y_pred)\n",
        "    print(f\"  Batch Size {batch_size:3d}: Accuracy = {acc:.4f}, Iterations = {mlp.n_iter_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comprehensive_example"
      },
      "source": [
        "### 3.5 - Putting It All Together: A Comprehensive Example\n",
        "\n",
        "Let's build an optimized neural network using all the concepts we've learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_realistic_data"
      },
      "outputs": [],
      "source": [
        "# Create a more complex dataset\n",
        "X_complex, y_complex = make_classification(\n",
        "    n_samples=2000, n_features=30, n_informative=20,\n",
        "    n_redundant=5, n_classes=3, n_clusters_per_class=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {X_complex.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_complex))}\")\n",
        "print(f\"Class distribution: {np.bincount(y_complex)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_final_data"
      },
      "outputs": [],
      "source": [
        "# Split and scale\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_complex, y_complex, test_size=0.2, random_state=42, stratify=y_complex\n",
        ")\n",
        "\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_optimized_model"
      },
      "outputs": [],
      "source": [
        "# Build an optimized neural network\n",
        "mlp_optimized = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50, 25),  # 3 hidden layers with decreasing size\n",
        "    activation='relu',                  # ReLU activation\n",
        "    solver='adam',                      # Adam optimizer\n",
        "    alpha=0.001,                       # L2 regularization\n",
        "    batch_size=64,                     # Mini-batch size\n",
        "    learning_rate_init=0.001,          # Learning rate\n",
        "    max_iter=1000,\n",
        "    early_stopping=True,               # Stop when validation performance plateaus\n",
        "    validation_fraction=0.1,           # Use 10% of training data for validation\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "mlp_optimized.fit(X_train_c_scaled, y_train_c)\n",
        "\n",
        "print(f\"Training completed in {mlp_optimized.n_iter_} iterations\")\n",
        "print(f\"Final training loss: {mlp_optimized.loss_:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_final_model"
      },
      "outputs": [],
      "source": [
        "# Evaluate the optimized model\n",
        "y_pred_c = mlp_optimized.predict(X_test_c_scaled)\n",
        "\n",
        "# Multi-class metrics\n",
        "accuracy = accuracy_score(y_test_c, y_pred_c)\n",
        "precision = precision_score(y_test_c, y_pred_c, average='weighted')\n",
        "recall = recall_score(y_test_c, y_pred_c, average='weighted')\n",
        "f1 = f1_score(y_test_c, y_pred_c, average='weighted')\n",
        "\n",
        "print(\"\\nOptimized Model Performance:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_confusion_matrix"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix for multi-class\n",
        "cm = confusion_matrix(y_test_c, y_pred_c)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[f'Class {i}' for i in range(3)],\n",
        "            yticklabels=[f'Class {i}' for i in range(3)])\n",
        "plt.title('Confusion Matrix - Optimized MLP on Multi-class Dataset')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_classification_report"
      },
      "outputs": [],
      "source": [
        "# Detailed report\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_c, y_pred_c, \n",
        "                          target_names=[f'Class {i}' for i in range(3)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, we covered:\n",
        "\n",
        "**Video 1: Perceptron & MLPs**\n",
        "- Single perceptrons for linearly separable data\n",
        "- Multi-layer perceptrons for non-linear patterns\n",
        "- The power of hidden layers\n",
        "\n",
        "**Video 2: MLPs 2 & Advanced Metrics**\n",
        "- Different network architectures and activation functions\n",
        "- Beyond accuracy: precision, recall, F1-score\n",
        "- Confusion matrices and classification reports\n",
        "- Handling imbalanced datasets\n",
        "\n",
        "**Video 3: Neural Networks Deep Dive**\n",
        "- How neural networks learn through backpropagation\n",
        "- Effect of learning rate and optimization algorithms\n",
        "- Regularization techniques to prevent overfitting\n",
        "- Batch size and training dynamics\n",
        "- Building an optimized neural network for multi-class classification"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
