{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprince0031/ICT-Python-ML/blob/main/Week%205/Notebooks/week5_reference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Python & ML Foundations: Session 5 - Reference\n",
        "## Perceptrons, MLPs & Advanced Metrics\n",
        "\n",
        "Welcome to the session 5 reference notebook! This week, we explore:\n",
        "- Perceptrons and their limitations (AND, OR, XOR problems)\n",
        "- Multi-layer perceptrons for regression and classification\n",
        "- Advanced evaluation metrics for imbalanced datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, r2_score\n",
        "from sklearn.datasets import load_diabetes, make_classification\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Video 1: Perceptrons and MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 - Perceptron: AND Operation\n",
        "\n",
        "A perceptron can learn linearly separable functions like AND."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AND operation dataset\n",
        "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "\n",
        "print(\"AND Truth Table:\")\n",
        "print(\"Input | Output\")\n",
        "print(\"------|-------\")\n",
        "for i in range(len(X_and)):\n",
        "    print(f\"{X_and[i]} | {y_and[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train perceptron on AND\n",
        "perceptron_and = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron_and.fit(X_and, y_and)\n",
        "\n",
        "# Test\n",
        "y_pred_and = perceptron_and.predict(X_and)\n",
        "print(\"\\nAND Perceptron Predictions:\")\n",
        "print(\"Input | Actual | Predicted\")\n",
        "print(\"------|--------|----------\")\n",
        "for i in range(len(X_and)):\n",
        "    print(f\"{X_and[i]} | {y_and[i]:6d} | {y_pred_and[i]:9d}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_and, y_pred_and):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 - Perceptron: OR Operation\n",
        "\n",
        "Similarly, a perceptron can learn the OR operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OR operation dataset\n",
        "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_or = np.array([0, 1, 1, 1])\n",
        "\n",
        "print(\"OR Truth Table:\")\n",
        "print(\"Input | Output\")\n",
        "print(\"------|-------\")\n",
        "for i in range(len(X_or)):\n",
        "    print(f\"{X_or[i]} | {y_or[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train perceptron on OR\n",
        "perceptron_or = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron_or.fit(X_or, y_or)\n",
        "\n",
        "# Test\n",
        "y_pred_or = perceptron_or.predict(X_or)\n",
        "print(\"\\nOR Perceptron Predictions:\")\n",
        "print(\"Input | Actual | Predicted\")\n",
        "print(\"------|--------|----------\")\n",
        "for i in range(len(X_or)):\n",
        "    print(f\"{X_or[i]} | {y_or[i]:6d} | {y_pred_or[i]:9d}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_or, y_pred_or):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 - Perceptron Limitation: XOR Problem\n",
        "\n",
        "The XOR problem is NOT linearly separable, so a single perceptron cannot solve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XOR operation dataset\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "print(\"XOR Truth Table:\")\n",
        "print(\"Input | Output\")\n",
        "print(\"------|-------\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"{X_xor[i]} | {y_xor[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to train perceptron on XOR (it will fail!)\n",
        "perceptron_xor = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# Test\n",
        "y_pred_xor = perceptron_xor.predict(X_xor)\n",
        "print(\"\\nXOR Perceptron Predictions (FAILS):\")\n",
        "print(\"Input | Actual | Predicted\")\n",
        "print(\"------|--------|----------\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"{X_xor[i]} | {y_xor[i]:6d} | {y_pred_xor[i]:9d}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_xor, y_pred_xor):.2f}\")\n",
        "print(\"\\n⚠️ The perceptron CANNOT solve XOR because it's not linearly separable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 - MLP Solves XOR Problem\n",
        "\n",
        "A multi-layer perceptron with hidden layers CAN solve the XOR problem!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLP with one hidden layer to solve XOR\n",
        "mlp_xor = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', \n",
        "                        max_iter=5000, random_state=42)\n",
        "mlp_xor.fit(X_xor, y_xor)\n",
        "\n",
        "# Test\n",
        "y_pred_mlp_xor = mlp_xor.predict(X_xor)\n",
        "print(\"\\nXOR MLP Predictions (SUCCESS):\")\n",
        "print(\"Input | Actual | Predicted\")\n",
        "print(\"------|--------|----------\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"{X_xor[i]} | {y_xor[i]:6d} | {y_pred_mlp_xor[i]:9d}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_xor, y_pred_mlp_xor):.2f}\")\n",
        "print(\"\\n✅ The MLP successfully solves XOR with hidden layers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Video 2: MLPs for Regression and Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 - Load Real-World Dataset\n",
        "\n",
        "We'll use the Diabetes dataset for regression and classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"Diabetes Dataset:\")\n",
        "print(f\"  Samples: {X_diabetes.shape[0]}\")\n",
        "print(f\"  Features: {X_diabetes.shape[1]}\")\n",
        "print(f\"  Target range: {y_diabetes.min():.2f} to {y_diabetes.max():.2f}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 - MLP Regressor WITHOUT ReLU (Identity Activation)\n",
        "\n",
        "Without non-linear activation, the MLP behaves like linear regression, limiting its ability to model non-linear relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLP with identity (linear) activation\n",
        "mlp_identity = MLPRegressor(hidden_layer_sizes=(10, 10), activation='identity',\n",
        "                            max_iter=1000, random_state=42)\n",
        "mlp_identity.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_identity = mlp_identity.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse_identity = mean_squared_error(y_test, y_pred_identity)\n",
        "r2_identity = r2_score(y_test, y_pred_identity)\n",
        "\n",
        "print(\"MLP Regressor with Identity (Linear) Activation:\")\n",
        "print(f\"  MSE: {mse_identity:.2f}\")\n",
        "print(f\"  R² Score: {r2_identity:.4f}\")\n",
        "print(\"\\n⚠️ Without non-linearity, the model struggles with complex patterns!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 - MLP Regressor WITH ReLU Activation\n",
        "\n",
        "Adding ReLU activation enables the MLP to model non-linear relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLP with ReLU activation\n",
        "mlp_relu = MLPRegressor(hidden_layer_sizes=(10, 10), activation='relu',\n",
        "                        max_iter=1000, random_state=42)\n",
        "mlp_relu.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_relu = mlp_relu.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse_relu = mean_squared_error(y_test, y_pred_relu)\n",
        "r2_relu = r2_score(y_test, y_pred_relu)\n",
        "\n",
        "print(\"MLP Regressor with ReLU Activation:\")\n",
        "print(f\"  MSE: {mse_relu:.2f}\")\n",
        "print(f\"  R² Score: {r2_relu:.4f}\")\n",
        "print(\"\\n✅ ReLU activation improves performance on non-linear data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Identity activation\n",
        "axes[0].scatter(y_test, y_pred_identity, alpha=0.6)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title(f'Identity Activation\\nR² = {r2_identity:.4f}')\n",
        "\n",
        "# Plot 2: ReLU activation\n",
        "axes[1].scatter(y_test, y_pred_relu, alpha=0.6, color='green')\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('Actual Values')\n",
        "axes[1].set_ylabel('Predicted Values')\n",
        "axes[1].set_title(f'ReLU Activation\\nR² = {r2_relu:.4f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nImprovement with ReLU: {(r2_relu - r2_identity):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 - MLP Classifier with Sigmoid Activation\n",
        "\n",
        "Now let's use the same dataset for classification by creating binary classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create binary classification problem from diabetes data\n",
        "# Classes: 0 = low progression, 1 = high progression\n",
        "median_value = np.median(y_diabetes)\n",
        "y_diabetes_binary = (y_diabetes > median_value).astype(int)\n",
        "\n",
        "print(f\"Binary classification threshold: {median_value:.2f}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(f\"  Class 0 (low): {np.sum(y_diabetes_binary == 0)} samples\")\n",
        "print(f\"  Class 1 (high): {np.sum(y_diabetes_binary == 1)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split for classification\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_diabetes, y_diabetes_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLP Classifier with sigmoid (logistic) activation\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10, 10), activation='logistic',\n",
        "                               max_iter=1000, random_state=42)\n",
        "mlp_classifier.fit(X_train_c_scaled, y_train_c)\n",
        "\n",
        "# Predictions\n",
        "y_pred_classifier = mlp_classifier.predict(X_test_c_scaled)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test_c, y_pred_classifier)\n",
        "\n",
        "print(\"MLP Classifier with Sigmoid Activation:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\n✅ Sigmoid activation is ideal for binary classification!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Video 3: Advanced Metrics for Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 - Create Imbalanced Dataset\n",
        "\n",
        "Let's create an imbalanced dataset to demonstrate why accuracy alone is insufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create imbalanced dataset (90% class 0, 10% class 1)\n",
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15,\n",
        "    n_redundant=5, n_classes=2, weights=[0.9, 0.1],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Imbalanced Dataset:\")\n",
        "unique, counts = np.unique(y_imb, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"  Class {cls}: {count:4d} samples ({count/len(y_imb)*100:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class imbalance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(['Class 0\\n(Majority)', 'Class 1\\n(Minority)'], counts, color=['skyblue', 'salmon'])\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution - Imbalanced Dataset')\n",
        "plt.ylim(0, max(counts) * 1.1)\n",
        "\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(i, count + 20, f'{count} ({count/len(y_imb)*100:.1f}%)', \n",
        "             ha='center', fontweight='bold')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n⚠️ With such imbalance, a naive classifier could achieve 90% accuracy\")\n",
        "print(\"   by simply predicting all samples as Class 0!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 - Train Model and Show Confusion Matrix\n",
        "\n",
        "Let's train a classifier and examine the confusion matrix to understand True Positives, True Negatives, False Positives, and False Negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split and scale\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb\n",
        ")\n",
        "\n",
        "scaler_imb = StandardScaler()\n",
        "X_train_imb_scaled = scaler_imb.fit_transform(X_train_imb)\n",
        "X_test_imb_scaled = scaler_imb.transform(X_test_imb)\n",
        "\n",
        "# Train classifier\n",
        "mlp_imb = MLPClassifier(hidden_layer_sizes=(20, 10), activation='relu',\n",
        "                        max_iter=1000, random_state=42)\n",
        "mlp_imb.fit(X_train_imb_scaled, y_train_imb)\n",
        "\n",
        "# Predictions\n",
        "y_pred_imb = mlp_imb.predict(X_test_imb_scaled)\n",
        "\n",
        "print(\"Model trained on imbalanced data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_imb, y_pred_imb)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Actual Class', fontsize=12)\n",
        "plt.xlabel('Predicted Class', fontsize=12)\n",
        "\n",
        "# Add annotations\n",
        "plt.text(0.5, -0.1, f'TN = {cm[0, 0]}\\n(True Negatives)', \n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
        "plt.text(1.5, -0.1, f'FP = {cm[0, 1]}\\n(False Positives)', \n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
        "plt.text(0.5, 1.1, f'FN = {cm[1, 0]}\\n(False Negatives)', \n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
        "plt.text(1.5, 1.1, f'TP = {cm[1, 1]}\\n(True Positives)', \n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Components:\")\n",
        "print(f\"  True Negatives (TN):  {cm[0, 0]} - Correctly predicted as Class 0\")\n",
        "print(f\"  False Positives (FP): {cm[0, 1]} - Incorrectly predicted as Class 1\")\n",
        "print(f\"  False Negatives (FN): {cm[1, 0]} - Incorrectly predicted as Class 0\")\n",
        "print(f\"  True Positives (TP):  {cm[1, 1]} - Correctly predicted as Class 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 - Calculate Metrics Manually\n",
        "\n",
        "Let's manually calculate precision, recall, and F1-score to understand what they mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract values from confusion matrix\n",
        "TN = cm[0, 0]\n",
        "FP = cm[0, 1]\n",
        "FN = cm[1, 0]\n",
        "TP = cm[1, 1]\n",
        "\n",
        "# Manual calculations\n",
        "accuracy_manual = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision_manual = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall_manual = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MANUAL METRIC CALCULATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n1. ACCURACY:\")\n",
        "print(f\"   Formula: (TP + TN) / (TP + TN + FP + FN)\")\n",
        "print(f\"   Calculation: ({TP} + {TN}) / ({TP} + {TN} + {FP} + {FN})\")\n",
        "print(f\"   Result: {accuracy_manual:.4f}\")\n",
        "\n",
        "print(\"\\n2. PRECISION:\")\n",
        "print(f\"   Formula: TP / (TP + FP)\")\n",
        "print(f\"   Meaning: Of all predicted positives, how many were correct?\")\n",
        "print(f\"   Calculation: {TP} / ({TP} + {FP})\")\n",
        "print(f\"   Result: {precision_manual:.4f}\")\n",
        "\n",
        "print(\"\\n3. RECALL (Sensitivity):\")\n",
        "print(f\"   Formula: TP / (TP + FN)\")\n",
        "print(f\"   Meaning: Of all actual positives, how many did we find?\")\n",
        "print(f\"   Calculation: {TP} / ({TP} + {FN})\")\n",
        "print(f\"   Result: {recall_manual:.4f}\")\n",
        "\n",
        "print(\"\\n4. F1-SCORE:\")\n",
        "print(f\"   Formula: 2 * (Precision * Recall) / (Precision + Recall)\")\n",
        "print(f\"   Meaning: Harmonic mean of precision and recall\")\n",
        "print(f\"   Calculation: 2 * ({precision_manual:.4f} * {recall_manual:.4f}) / ({precision_manual:.4f} + {recall_manual:.4f})\")\n",
        "print(f\"   Result: {f1_manual:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify with sklearn functions\n",
        "accuracy_sklearn = accuracy_score(y_test_imb, y_pred_imb)\n",
        "precision_sklearn = precision_score(y_test_imb, y_pred_imb)\n",
        "recall_sklearn = recall_score(y_test_imb, y_pred_imb)\n",
        "f1_sklearn = f1_score(y_test_imb, y_pred_imb)\n",
        "\n",
        "print(\"\\nVerification (sklearn vs manual):\")\n",
        "print(f\"  Accuracy:  {accuracy_sklearn:.4f} vs {accuracy_manual:.4f}\")\n",
        "print(f\"  Precision: {precision_sklearn:.4f} vs {precision_manual:.4f}\")\n",
        "print(f\"  Recall:    {recall_sklearn:.4f} vs {recall_manual:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_sklearn:.4f} vs {f1_manual:.4f}\")\n",
        "print(\"\\n✅ Manual calculations match sklearn!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 - Classification Report\n",
        "\n",
        "The classification report provides a comprehensive view of all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test_imb, y_pred_imb, \n",
        "                          target_names=['Class 0 (Majority)', 'Class 1 (Minority)']))\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nKey Takeaways:\")\n",
        "print(\"  - Accuracy alone can be misleading with imbalanced data\")\n",
        "print(\"  - Precision tells us about false positive rate\")\n",
        "print(\"  - Recall tells us about false negative rate\")\n",
        "print(\"  - F1-score balances both precision and recall\")\n",
        "print(\"  - For imbalanced data, focus on minority class metrics!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Video 1: Perceptrons and MLPs\n",
        "- Perceptrons can solve AND and OR (linearly separable)\n",
        "- Perceptrons CANNOT solve XOR (not linearly separable)\n",
        "- MLPs with hidden layers solve XOR successfully\n",
        "\n",
        "### Video 2: MLPs for Regression and Classification\n",
        "- MLP without non-linear activation = limited to linear relationships\n",
        "- ReLU activation enables modeling of non-linear patterns\n",
        "- Sigmoid activation is ideal for binary classification\n",
        "\n",
        "### Video 3: Advanced Metrics\n",
        "- Accuracy can be misleading with imbalanced data\n",
        "- Confusion matrix shows TP, TN, FP, FN\n",
        "- Precision: Of predicted positives, how many correct?\n",
        "- Recall: Of actual positives, how many found?\n",
        "- F1-score: Harmonic mean of precision and recall\n",
        "- Classification report provides comprehensive metrics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
