{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprince0031/ICT-Python-ML/blob/main/Week%205/Notebooks/week5_solutions.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Python and ML Foundations: Session 5 - Solutions\n",
        "## Perceptrons, MLPs & Advanced Metrics\n",
        "\n",
        "Complete solutions for session 5 practice challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utility_header",
      "metadata": {},
      "source": [
        "## Utility code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, r2_score\n",
        "from sklearn.datasets import load_diabetes, make_classification, fetch_california_housing\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "video1_header",
      "metadata": {},
      "source": [
        "---\n",
        "## Video 1: Perceptrons and MLPs - NAND Gate Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video1_ex1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Create the NAND dataset\n",
        "X_nand = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_nand = np.array([1, 1, 1, 0])\n",
        "\n",
        "print(\"NAND Truth Table:\")\n",
        "print(\"Input | Output\")\n",
        "print(\"------|-------\")\n",
        "for i in range(len(X_nand)):\n",
        "    print(f\"{X_nand[i]} | {y_nand[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video1_ex2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Train a Perceptron on NAND\n",
        "perceptron_nand = Perceptron(max_iter=1000, random_state=42)\n",
        "perceptron_nand.fit(X_nand, y_nand)\n",
        "\n",
        "# Test\n",
        "y_pred_perceptron = perceptron_nand.predict(X_nand)\n",
        "\n",
        "print(\"\\nPerceptron Predictions on NAND:\")\n",
        "print(\"Input | Actual | Predicted\")\n",
        "print(\"------|--------|----------\")\n",
        "for i in range(len(X_nand)):\n",
        "    print(f\"{X_nand[i]} | {y_nand[i]:6d} | {y_pred_perceptron[i]:9d}\")\n",
        "\n",
        "acc_perceptron = accuracy_score(y_nand, y_pred_perceptron)\n",
        "print(f\"\\nPerceptron Accuracy: {acc_perceptron:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video1_ex3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Train an MLP on NAND\n",
        "mlp_nand = MLPClassifier(hidden_layer_sizes=(4,), activation='relu',\n",
        "                         max_iter=5000, random_state=42)\n",
        "mlp_nand.fit(X_nand, y_nand)\n",
        "\n",
        "# Test\n",
        "y_pred_mlp = mlp_nand.predict(X_nand)\n",
        "\n",
        "print(\"\\nMLP Predictions on NAND:\")\n",
        "print(\"Input | Actual | Predicted\")\n",
        "print(\"------|--------|----------\")\n",
        "for i in range(len(X_nand)):\n",
        "    print(f\"{X_nand[i]} | {y_nand[i]:6d} | {y_pred_mlp[i]:9d}\")\n",
        "\n",
        "acc_mlp = accuracy_score(y_nand, y_pred_mlp)\n",
        "print(f\"\\nMLP Accuracy: {acc_mlp:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video1_ex4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4: Compare results\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON: Perceptron vs MLP on NAND\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Perceptron Accuracy: {acc_perceptron:.2f}\")\n",
        "print(f\"MLP Accuracy:        {acc_mlp:.2f}\")\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"  ✅ Both models successfully solve NAND\")\n",
        "print(\"  ✅ NAND is linearly separable\")\n",
        "print(\"  ✅ A simple perceptron is sufficient for NAND\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "video2_header",
      "metadata": {},
      "source": [
        "---\n",
        "## Video 2: MLPs for Regression - California Housing Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video2_ex1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X_cal = california.data\n",
        "y_cal = california.target\n",
        "\n",
        "print(\"California Housing Dataset:\")\n",
        "print(f\"  Samples: {X_cal.shape[0]}\")\n",
        "print(f\"  Features: {X_cal.shape[1]}\")\n",
        "print(f\"  Target (median house value): ${y_cal.min():.2f} to ${y_cal.max():.2f} (in $100k)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video2_ex2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Split and scale\n",
        "X_train_cal, X_test_cal, y_train_cal, y_test_cal = train_test_split(\n",
        "    X_cal, y_cal, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_cal = StandardScaler()\n",
        "X_train_cal_scaled = scaler_cal.fit_transform(X_train_cal)\n",
        "X_test_cal_scaled = scaler_cal.transform(X_test_cal)\n",
        "\n",
        "print(f\"Training samples: {X_train_cal.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_cal.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video2_ex3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Train MLP with identity activation\n",
        "mlp_identity = MLPRegressor(hidden_layer_sizes=(20, 10), activation='identity',\n",
        "                            max_iter=1000, random_state=42)\n",
        "mlp_identity.fit(X_train_cal_scaled, y_train_cal)\n",
        "\n",
        "y_pred_identity = mlp_identity.predict(X_test_cal_scaled)\n",
        "r2_identity = r2_score(y_test_cal, y_pred_identity)\n",
        "mse_identity = mean_squared_error(y_test_cal, y_pred_identity)\n",
        "\n",
        "print(\"MLP with Identity Activation:\")\n",
        "print(f\"  R² Score: {r2_identity:.4f}\")\n",
        "print(f\"  MSE: {mse_identity:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video2_ex4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4: Train MLP with tanh activation\n",
        "mlp_tanh = MLPRegressor(hidden_layer_sizes=(20, 10), activation='tanh',\n",
        "                        max_iter=1000, random_state=42)\n",
        "mlp_tanh.fit(X_train_cal_scaled, y_train_cal)\n",
        "\n",
        "y_pred_tanh = mlp_tanh.predict(X_test_cal_scaled)\n",
        "r2_tanh = r2_score(y_test_cal, y_pred_tanh)\n",
        "mse_tanh = mean_squared_error(y_test_cal, y_pred_tanh)\n",
        "\n",
        "print(\"MLP with Tanh Activation:\")\n",
        "print(f\"  R² Score: {r2_tanh:.4f}\")\n",
        "print(f\"  MSE: {mse_tanh:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video2_ex5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 5: Compare R² scores\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON: Identity vs Tanh Activation\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Identity R²: {r2_identity:.4f}\")\n",
        "print(f\"Tanh R²:     {r2_tanh:.4f}\")\n",
        "print(f\"\\nImprovement: {(r2_tanh - r2_identity):.4f}\")\n",
        "print(\"\\n✅ Non-linear activation (tanh) performs better!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video2_ex6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 6: Visualize predictions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Identity activation plot\n",
        "axes[0].scatter(y_test_cal, y_pred_identity, alpha=0.5)\n",
        "axes[0].plot([y_test_cal.min(), y_test_cal.max()], \n",
        "             [y_test_cal.min(), y_test_cal.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title(f'Identity Activation\\nR² = {r2_identity:.4f}')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Tanh activation plot\n",
        "axes[1].scatter(y_test_cal, y_pred_tanh, alpha=0.5, color='green')\n",
        "axes[1].plot([y_test_cal.min(), y_test_cal.max()], \n",
        "             [y_test_cal.min(), y_test_cal.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('Actual Values')\n",
        "axes[1].set_ylabel('Predicted Values')\n",
        "axes[1].set_title(f'Tanh Activation\\nR² = {r2_tanh:.4f}')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  - Red dashed line = perfect predictions\")\n",
        "print(\"  - Points closer to line = better predictions\")\n",
        "print(\"  - Tanh shows better fit to actual values\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "video3_header",
      "metadata": {},
      "source": [
        "---\n",
        "## Video 3: Advanced Metrics - Fraud Detection Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video3_ex1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Create imbalanced dataset\n",
        "X_fraud, y_fraud = make_classification(\n",
        "    n_samples=10000, n_features=20, n_informative=15,\n",
        "    n_redundant=5, n_classes=2, weights=[0.98, 0.02],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Credit Card Fraud Dataset:\")\n",
        "unique, counts = np.unique(y_fraud, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    label = \"Legitimate\" if cls == 0 else \"Fraudulent\"\n",
        "    print(f\"  {label} (Class {cls}): {count:5d} samples ({count/len(y_fraud)*100:5.1f}%)\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(['Legitimate\\n(Class 0)', 'Fraudulent\\n(Class 1)'], counts, \n",
        "        color=['lightgreen', 'salmon'])\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.title('Class Distribution - Highly Imbalanced Dataset')\n",
        "plt.ylim(0, max(counts) * 1.1)\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(i, count + 100, f'{count} ({count/len(y_fraud)*100:.1f}%)', \n",
        "             ha='center', fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video3_ex2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Train MLP classifier\n",
        "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
        "    X_fraud, y_fraud, test_size=0.3, random_state=42, stratify=y_fraud\n",
        ")\n",
        "\n",
        "scaler_fraud = StandardScaler()\n",
        "X_train_fraud_scaled = scaler_fraud.fit_transform(X_train_fraud)\n",
        "X_test_fraud_scaled = scaler_fraud.transform(X_test_fraud)\n",
        "\n",
        "mlp_fraud = MLPClassifier(hidden_layer_sizes=(30, 20), activation='relu',\n",
        "                          max_iter=1000, random_state=42)\n",
        "mlp_fraud.fit(X_train_fraud_scaled, y_train_fraud)\n",
        "\n",
        "y_pred_fraud = mlp_fraud.predict(X_test_fraud_scaled)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Training samples: {X_train_fraud.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_fraud.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video3_ex3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 3: Confusion matrix\n",
        "cm_fraud = confusion_matrix(y_test_fraud, y_pred_fraud)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_fraud, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Legitimate', 'Predicted Fraud'],\n",
        "            yticklabels=['Actual Legitimate', 'Actual Fraud'])\n",
        "plt.title('Confusion Matrix - Fraud Detection', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Actual Class', fontsize=12)\n",
        "plt.xlabel('Predicted Class', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Components:\")\n",
        "print(f\"  True Negatives (TN):  {cm_fraud[0, 0]:5d} - Correctly identified legitimate\")\n",
        "print(f\"  False Positives (FP): {cm_fraud[0, 1]:5d} - Legitimate flagged as fraud\")\n",
        "print(f\"  False Negatives (FN): {cm_fraud[1, 0]:5d} - Fraud missed (dangerous!)\")\n",
        "print(f\"  True Positives (TP):  {cm_fraud[1, 1]:5d} - Correctly identified fraud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video3_ex4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 4: Manual calculations\n",
        "TN = cm_fraud[0, 0]\n",
        "FP = cm_fraud[0, 1]\n",
        "FN = cm_fraud[1, 0]\n",
        "TP = cm_fraud[1, 1]\n",
        "\n",
        "accuracy_manual = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision_manual = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "recall_manual = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MANUAL METRIC CALCULATIONS FOR FRAUD DETECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. ACCURACY:\")\n",
        "print(f\"   Formula: (TP + TN) / Total\")\n",
        "print(f\"   Calculation: ({TP} + {TN}) / {TP + TN + FP + FN}\")\n",
        "print(f\"   Result: {accuracy_manual:.4f} ({accuracy_manual*100:.2f}%)\")\n",
        "print(f\"   ⚠️  High accuracy can be misleading with imbalanced data!\")\n",
        "\n",
        "print(\"\\n2. PRECISION (Fraud Class):\")\n",
        "print(f\"   Formula: TP / (TP + FP)\")\n",
        "print(f\"   Meaning: Of all predicted frauds, how many were actually fraud?\")\n",
        "print(f\"   Calculation: {TP} / ({TP} + {FP})\")\n",
        "print(f\"   Result: {precision_manual:.4f} ({precision_manual*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n3. RECALL (Fraud Class):\")\n",
        "print(f\"   Formula: TP / (TP + FN)\")\n",
        "print(f\"   Meaning: Of all actual frauds, how many did we catch?\")\n",
        "print(f\"   Calculation: {TP} / ({TP} + {FN})\")\n",
        "print(f\"   Result: {recall_manual:.4f} ({recall_manual*100:.2f}%)\")\n",
        "print(f\"   ⚠️  Critical metric! Missing fraud is costly!\")\n",
        "\n",
        "print(\"\\n4. F1-SCORE (Fraud Class):\")\n",
        "print(f\"   Formula: 2 * (Precision * Recall) / (Precision + Recall)\")\n",
        "print(f\"   Meaning: Harmonic mean balancing precision and recall\")\n",
        "print(f\"   Result: {f1_manual:.4f} ({f1_manual*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Verify\n",
        "accuracy_sklearn = accuracy_score(y_test_fraud, y_pred_fraud)\n",
        "precision_sklearn = precision_score(y_test_fraud, y_pred_fraud)\n",
        "recall_sklearn = recall_score(y_test_fraud, y_pred_fraud)\n",
        "f1_sklearn = f1_score(y_test_fraud, y_pred_fraud)\n",
        "\n",
        "print(\"\\nVerification (sklearn):\")\n",
        "print(f\"  Accuracy:  {accuracy_sklearn:.4f} ✓\")\n",
        "print(f\"  Precision: {precision_sklearn:.4f} ✓\")\n",
        "print(f\"  Recall:    {recall_sklearn:.4f} ✓\")\n",
        "print(f\"  F1-Score:  {f1_sklearn:.4f} ✓\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "video3_ex5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 5: Classification report\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*70)\n",
        "print(classification_report(y_test_fraud, y_pred_fraud,\n",
        "                          target_names=['Legitimate', 'Fraudulent']))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "video3_discussion",
      "metadata": {},
      "source": [
        "### Task 6: Discussion - Answers\n",
        "\n",
        "**1. Which metric is most important for fraud detection?**\n",
        "\n",
        "Answer: **RECALL** is the most critical metric for fraud detection. Here's why:\n",
        "- Recall measures how many fraudulent transactions we successfully detect\n",
        "- Missing a fraudulent transaction (False Negative) can result in significant financial losses\n",
        "- It's better to flag some legitimate transactions as suspicious (False Positives) than to miss actual fraud\n",
        "- High recall ensures we catch most fraud cases, even if we have some false alarms\n",
        "\n",
        "**2. Why is accuracy not sufficient for this problem?**\n",
        "\n",
        "Answer: Accuracy is misleading with imbalanced data:\n",
        "- With 98% legitimate transactions, a naive model that predicts everything as \"legitimate\" achieves 98% accuracy\n",
        "- This model would catch ZERO fraud cases but still have \"high accuracy\"\n",
        "- Accuracy doesn't tell us how well we detect the minority class (fraud)\n",
        "- For imbalanced datasets, we need metrics that focus on the minority class performance\n",
        "\n",
        "**3. What's worse in fraud detection: False Positives or False Negatives?**\n",
        "\n",
        "Answer: **False Negatives are worse:**\n",
        "- False Negative = Missing actual fraud (letting fraud go through)\n",
        "  - Results in direct financial losses\n",
        "  - Damages customer trust\n",
        "  - Can lead to larger fraud patterns\n",
        "\n",
        "- False Positive = Flagging legitimate transaction as fraud\n",
        "  - Causes customer inconvenience\n",
        "  - Requires manual review\n",
        "  - But no financial loss\n",
        "\n",
        "**4. How would you improve the model to better detect fraud?**\n",
        "\n",
        "Possible improvements:\n",
        "- **Class weights**: Penalize fraud misclassification more heavily\n",
        "- **Resampling**: Use SMOTE or undersampling to balance classes\n",
        "- **Ensemble methods**: Use Random Forest or XGBoost\n",
        "- **Threshold tuning**: Lower the classification threshold to increase recall\n",
        "- **Feature engineering**: Create fraud-specific features (transaction patterns, time, amount)\n",
        "- **Anomaly detection**: Use isolation forests or autoencoders\n",
        "- **Cost-sensitive learning**: Assign different costs to different types of errors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Video 1: Perceptrons and MLPs\n",
        "- NAND gate is linearly separable, solvable by both Perceptron and MLP\n",
        "- Perceptrons work for simple linear problems\n",
        "- MLPs add flexibility with hidden layers\n",
        "\n",
        "### Video 2: MLPs for Regression\n",
        "- Identity (linear) activation limits the model to linear relationships\n",
        "- Non-linear activations (tanh, ReLU) enable modeling complex patterns\n",
        "- R² score improved significantly with non-linear activation\n",
        "\n",
        "### Video 3: Advanced Metrics\n",
        "- Accuracy is misleading with imbalanced data\n",
        "- Confusion matrix reveals TP, TN, FP, FN\n",
        "- Precision = TP / (TP + FP) - \"How many predicted positives are correct?\"\n",
        "- Recall = TP / (TP + FN) - \"How many actual positives did we find?\"\n",
        "- F1-score balances precision and recall\n",
        "- For fraud detection, prioritize **RECALL** to minimize missed frauds\n",
        "- Classification report provides comprehensive metrics for all classes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
